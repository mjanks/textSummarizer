Abstract
Reverse engineering techniques have the potential to support Web site understanding, by providing views that show the organization of a site and its navigational struc- ture. However, representing each Web page as a node in the diagrams that are recovered from the source code of a Web site leads often to huge and unreadable graphs. Moreover, since the level of connectivity is typically high, the edges in such graphs make the overall result still less usable.
Clustering can be used to produce cohesive groups of pages that are displayed as a single node in reverse engi- neered diagrams. In this paper, we propose a clustering method based on the automatic extraction of the keywords of a Web page. The presence of common keywords is ex- ploited to decide when it is appropriate to group pages to- gether. A second usage of the keywords is in the automatic labeling of the recovered clusters of pages.
1 Introduction
Web sites often evolve from small and simple collections of purely HTML pages to big and complex applications, of- fering advanced transaction and data access facilities. The navigation structure is also subject to a similar trend. While initially only a few navigation facilities are needed, as the complexity grows, more advanced and intricate connections are provided. Lack of an established Web engineering prac- tice is often the reason for a drift in the overall organization of the Web site. Cumulative maintenance interventions and successive radical changes may result in a “legacy” Web site, that can be hardly evolved in a safe and controlled way.
Tools and techniques are being developed to support understanding and restructuring of existing Web applica- tions [4, 8, 11]. Software clustering [1, 6] aims at gather- ing software components into higher level groupings, thus providing the user with a more abstract, overall view of the system under analysis. It can be similarly adapted to the
Web context, in order to produce a high level view of the Web site organization, in terms of cohesive groups (clus- ters) of pages and of the relationships among them. Such a view can be exploited in the process of Web site under- standing, to gain knowledge about the organization of the entire site. More detailed information can be obtained by exploding each cluster of interest into subclusters, up to the individual pages that are the object of the change.
Web site clustering involves several non obvious deci- sions that profoundly affect the final result. First of all, the features used to describe each Web page have to be deter- mined. These are the basic properties used to measure the similarity between pages, which in turn determines when two pages are clustered together. Several completely differ- ent choices are possible, ranging from the Web site connec- tivity [4], to the page structure [9], or to the content. The precise similarity measure, as well as the clustering algo- rithm in use, are other important parameters.
After computing the clusters for a Web site, a Web devel- oper can access them to understand the overall organization of the system. However, to be meaningful, clusters should be properly labeled. A high level view showing just blank boxes (clusters) connected with each other, or, at the other extreme, labeled with all included pages, is by no means very informative and helpful. On the other side, a man- ual labeling process (concept assignment [4]) might be very difficult and time consuming. Some degree of automatic cluster labeling is thus another crucial feature for a practi- cal usage of clustering in Web site evolution.
In this paper, we consider the page content as the basic feature used to cluster a Web site. Summary information about a page is obtained by means of keyword extraction. The same technique is exploited to attack the problem of cluster labeling. The keyword with the highest score within each cluster is used as cluster label. Preliminary experimen- tal results confirm the feasibility of the approach.
The paper is organized as follows: the next section con- trasts the existing literature with our proposal. Then, a brief summary on clustering methods is provided to make the pa-
per self-contained (Section 3). In Section 4, the Natural Language Processing (NLP) method of keyword extraction is presented. The next section describes our clustering algo- rithm based on the keywords associated to each Web page, as well as our automatic cluster labeling technique. A case study is commented in Section 6. Conclusions and future work are given in the last section.
ters). Given a system consisting of entities which are char- acterized by a vector of properties and are connected by mu- tual relationships, there are two main approaches to cluster- ing [1]: the sibling link and the direct link approach. In the sibling link approach, entities are grouped together when they possess similar properties, while in the direct link ap- proach they are grouped together when the mutual relation- ships form a highly interconnected sub-graph.
In the literature there exist several different clustering al- gorithms [12], with different properties. Hierarchical algo- rithms do not produce a single partition of the system. Their output is rather a tree, with the root consisting of one cluster enclosing all entities, and the leaves consisting of single- ton clusters. At each intermediate level, a partition of the system is available, with the number of clusters increasing while moving downward in the tree. Divisive algorithms start from the whole system at the tree root, and then divide it into smaller clusters, attached as tree children. Alterna- tively, agglomerative algorithms start from singleton clus- ters and join them together incrementally.
3.1 Agglomerative hierarchical clustering
The agglomerative hierarchical clustering algorithm builds a hierarchy of clusterings starting from the bottom of the hierarchy, where each entity is in a different clus- ter. In each following step, the two most similar clusters are joined. After       steps (with   the number of enti- ties), all entities are grouped into one cluster. Each level in the hierarchy defines a partition of clusters (i.e., a cluster- ing). To select the resulting clustering, a cut point has to be determined.
We have adapted the agglomerative hierarchical algo- rithm known as Johnson’s algorithm [12] to our purposes:
1. begin with   clusters, each containing one Web page (  is the number of Web pages in the Web site), and compute distance between pages (using the comple- ment of the similarity measure).
2. while there are more than 1 cluster do
(a) find the pair of clusters at least distance;
(b) merge these clusters into a new cluster;
(c) update the distance measures between each pair of clusters.
end while
To update the distance measure between clusters, we have chosen the so called complete linkage rule [1]. This rule states that the distance measure between the already existing cluster   and a new cluster   , formed by joining clusters   and   , is the minimum between dist(     ) and dist(      ). It privileges cohesion over coupling [1].
2
Related work
Clustering has several uses in program understanding and software reengineering [1, 6, 12], and has been recently applied to Web applications [4, 5, 9].
In [4] an approach to support the comprehension of Web applications by exploiting clustering techniques has been proposed. The approach is based on a conceptual model of a Web application and on a similarity measure between com- ponents that takes into account both the type and the topol- ogy of the links. This measure is exploited by a hierarchical clustering algorithm which produces a hierarchy of system partitions. Download and comprehension of the considered Web applications are conducted using the reverse engineer- ing tool WARE.
Our approach introduces one major improvement over the technique described in [4]: automatic cluster labeling, and differs mainly in one respect: the basic feature exploited for clustering, which is the page content instead of its con- nectivity. In fact, content plays a very important role in Web applications and it can be hypothesized that it represents a good starting point for clustering. On the other side, the connectivity of a Web application suffers from a problem, highlighted also by the authors of [4]: purely navigational links (such as those leading to the home page) can hardly be distinguished from semantically richer ones. Labels in [4] (named concepts) are assigned to clusters manually. In our approach, labels assignment is handled in a completely au- tomatic way.
In [5] an approach to identify duplicated pages (i.e., clones) in a Web application is proposed. Two different methods based on different similarity measures have been defined and experimented with: one exploiting the edit distance and the other one based on the frequency of the HTML tags in a page. The underlying descriptive feature (the HTML structure of a page) can be used as a further basic feature for clustering. This feature was actually ex- ploited in [9], where an approach is presented for the iden- tification of Web pages that can be migrated to a dynamic version, in that they share a similar structure.
3 Clustering
Clustering is a general technique aimed at gathering the entities that compose a system into cohesive groups (clus-
4 Keyword extraction
To the aim of clustering Web pages having similar con- tent, we need to characterize the content of a document in a way that is simple and computationally tractable. Also, the representation of the text content must be suitable for calcu- lating efficiently content similarity measures between texts. A list of the most relevant keywords in the document can be used for this purpose. This approach is used for instance in [10].